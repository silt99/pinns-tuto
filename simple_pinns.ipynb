{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlEMv4srrxSX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import grad, Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6MbrvoYrxSY"
      },
      "outputs": [],
      "source": [
        "#Lets Create a Simple NN (MLP, 3 layers, RELU)\n",
        "class SimpleNN(nn.Sequential):\n",
        "    \"\"\"SimpleNN.\n",
        "\n",
        "    Args:\n",
        "        input_size  (int): Input size.\n",
        "        hidden_size (int): Hidden size.\n",
        "        output_size (int): Output size.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h2UkgNNrxSY"
      },
      "outputs": [],
      "source": [
        "# Lets Define the Physics Loss\n",
        "# We need to store one paramer (velocity)\n",
        "# The formula is df/dt=v*df/dx\n",
        "class SinPhyLoss(nn.Module):\n",
        "    \"\"\"SinPhy loss.\n",
        "\n",
        "    Args:\n",
        "        velocity (float): velocity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, velocity):\n",
        "        super().__init__()\n",
        "        self.velocity = velocity\n",
        "\n",
        "    def forward(self, pY, t, x, weight=None, **kwargs):\n",
        "        \"\"\"Forward Function.\n",
        "\n",
        "        Args:\n",
        "            pY (Tensor): of shape (N, C, H, W). Predicted tensor.\n",
        "            x (Tensor): of shape (N, C, H, W).  Input truth tensor.\n",
        "            weight (Tensor, optional): of shape (N, C, H, W). Element-wise\n",
        "                weights. Default: None.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHX5Ak1DrxSZ"
      },
      "outputs": [],
      "source": [
        "#Lets new define a train step\n",
        "def train_step(fx, train_data, train_phys, train_bound,\n",
        "               loss_phys_func,\n",
        "               data_coef=1, phys_coef=1, bound_coef=1):\n",
        "    \"\"\"Train step.\n",
        "\n",
        "    Args:\n",
        "        fx (nn.Module): Network.\n",
        "        train_data (Tensor):  of shape (N, 3, H, W). Train data (regression) with t, x and f.\n",
        "        train_phys (Tensor):  of shape (M, 3, H, W). Train phys data with t and x.\n",
        "        train_bound (Tensor): of shape (L, 3, H, W). Train bound data points with t, x and f.\n",
        "        loss_phys_func (nn.Module): Physics loss.\n",
        "        data_coef (float):  Weight of the regression term in the loss. Default 1.\n",
        "        phys_coef (float):  Weight of the physiscs term in the loss. Default 1.\n",
        "        bound_coef (float): Weight of the bound conditions term in the loss. Default 1.\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNpbF-garxSZ"
      },
      "outputs": [],
      "source": [
        "#Util for batch sampling\n",
        "def batch_sample(data, batch_size):\n",
        "    indices = torch.randint(low=0, high=data.shape[0], size=(batch_size,))\n",
        "\n",
        "    return data[indices]\n",
        "\n",
        "def domain_sampling(batch_size, T, L):\n",
        "    t = torch.rand(size=(batch_size,))*T\n",
        "    x = torch.rand(size=(batch_size,))*L\n",
        "\n",
        "    return torch.stack((t, x), dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYZ16UZXrxSa"
      },
      "outputs": [],
      "source": [
        "def gen_data(L=1.0, T=2.0, v=0.1, Nx=101, Nt=1000):\n",
        "    # Parameters\n",
        "    L = 1.0  # Length of the string\n",
        "    T = 2.0  # Total time of simulation\n",
        "    v = 0.1  # Wave speed\n",
        "    Nx = 101  # Number of spatial points\n",
        "    Nt = 1000  # Number of time steps\n",
        "\n",
        "    # Derived parameters\n",
        "    dx = L / (Nx - 1)  # Spatial step size\n",
        "    dt = T / Nt  # Time step size\n",
        "    c = v * dt / dx  # Courant number\n",
        "\n",
        "    if c > 1:\n",
        "        raise ValueError(\"The Courant number must be <= 1 for stability.\")\n",
        "\n",
        "    # Spatial and temporal grids\n",
        "    x = np.linspace(0, L, Nx)\n",
        "    t = np.linspace(0, T, Nt)\n",
        "\n",
        "    # Initial conditions\n",
        "    f = np.zeros((Nt, Nx))  # Initialize the solution array\n",
        "\n",
        "    # Boundary conditions (fixed ends)\n",
        "    f[:, 0] = 0\n",
        "    f[:, -1] = 0\n",
        "\n",
        "    # Initial condition at t = 0\n",
        "    # OPTION A: Considering the string is already vibrating\n",
        "    f[0, :] = np.sin(10 * np.pi * x)\n",
        "\n",
        "    # OPTION B: Considering we just hit the string with our thumb in the center (Gaussian model at a point point)\n",
        "    # mu = L/2\n",
        "    # sigma = L/20\n",
        "    # f[0, :] = np.exp(-(x - mu)**2/(2 * sigma**2))\n",
        "\n",
        "    # Assume zero initial velocity to start the numerical propagation\n",
        "    f[1, :] = f[0, :]\n",
        "\n",
        "\n",
        "    # Finite difference scheme\n",
        "    for n in range(1, Nt - 1):\n",
        "        f[n + 1, 1:-1] = (\n",
        "            2 * f[n, 1:-1]\n",
        "            - f[n - 1, 1:-1]\n",
        "            + c**2 * (f[n, 2:] - 2 * f[n, 1:-1] + f[n, :-2])\n",
        "        )\n",
        "\n",
        "    X = np.zeros((t.shape[0], x.shape[0], 2), np.float32)\n",
        "    y = np.zeros((t.shape[0], x.shape[0]), np.float32)\n",
        "    boundary = np.zeros((t.shape[0]*2, 3), np.float32)\n",
        "\n",
        "    for i, ti in enumerate(t):\n",
        "        for j, xj in enumerate(x):\n",
        "            X[i, j, 0] = ti\n",
        "            X[i, j, 1] = xj\n",
        "            y[i, j] = f[i, j]\n",
        "\n",
        "    boundary[:t.shape[0], 0] = t\n",
        "    boundary[t.shape[0]:, 0] = t\n",
        "    boundary[:t.shape[0], 1] = x[0]\n",
        "    boundary[t.shape[0]:, 1] = x[-1]\n",
        "\n",
        "    X = X.reshape((t.shape[0]*x.shape[0], 2))\n",
        "    y = y.reshape((t.shape[0]*x.shape[0], 1))\n",
        "\n",
        "    return X, y, boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_rE1oaQrxSa"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def animate_wave(f, tx, T=2.0, Nt=1000, interval=200):\n",
        "    dt = T / Nt\n",
        "\n",
        "    #plt.figure(figsize=(8, 4))\n",
        "    for n in range(0, Nt, interval):\n",
        "        t = n * dt\n",
        "        idx_t = np.abs(tx[:, 0]-t) < dt\n",
        "\n",
        "        x_t = tx[idx_t, 1]\n",
        "        f_t = f[idx_t, 0]\n",
        "\n",
        "        #plt.clf()\n",
        "        plt.plot(x_t, f_t, label=f\"Time: {n * dt:.2f} s\")\n",
        "        #plt.pause(0.01)\n",
        "    plt.ylim(-1.2, 1.2)\n",
        "    plt.xlabel(\"Position (x)\")\n",
        "    plt.ylabel(\"Displacement (f)\")\n",
        "    plt.title(\"1D Wave Equation Simulation\")\n",
        "    #plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1nCTw_srxSa"
      },
      "outputs": [],
      "source": [
        "#Some common paramerters\n",
        "RS = 1    #Random Seed\n",
        "L = 1.0   # Length of the string\n",
        "T = 2.0   # Total time of simulation\n",
        "v = 0.1   # Wave speed\n",
        "Nx = 101  # Number of spatial points\n",
        "Nt = 1000 # Number of time steps\n",
        "N  = int(1e5)  # Number of training iterations\n",
        "BS = 256  # Batch size\n",
        "PN = 100  # Print results every this number\n",
        "\n",
        "#Set Random seed for numpy and torch\n",
        "np.random.seed(RS)\n",
        "g=torch.manual_seed(RS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM9ExtjFrxSb"
      },
      "outputs": [],
      "source": [
        "#First, generate data and boundary\n",
        "\n",
        "#Lets get some random points for training\n",
        "NTrain = int(0.5*tx.shape[0]) # We are going to use 50% of points for training in this example\n",
        "\n",
        "#Shuffle!\n",
        "ind_train = np.arange(0,tx.shape[0],1,dtype=np.int32)\n",
        "np.random.shuffle(ind_train)\n",
        "ind_train = ind_train[:NTrain]\n",
        "\n",
        "#Create tensors\n",
        "train_data = torch.from_numpy(np.concatenate([tx[ind_train], f[ind_train]], axis=1))\n",
        "train_boundary = torch.from_numpy(boundary)\n",
        "\n",
        "#Second, let's create the physics loss\n",
        "loss_phys_func = SinPhyLoss(velocity=v)\n",
        "\n",
        "#Third, network\n",
        "network = SimpleNN(input_size=2, hidden_size=512, output_size=1)\n",
        "\n",
        "#Finally, Adam\n",
        "optimizer = Adam(network.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfwG9vPvrxSc",
        "outputId": "7baa9be3-4751-4b62-9ea5-90c066542859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 0, Loss: 0.17664356529712677,                 Loss data: 0.12789037823677063,                 Loss boundary: 0.0023139941040426493,                 Loss phys: 0.04643918573856354\n",
            "Iter 100, Loss: 0.16609232127666473,                 Loss data: 0.12222839891910553,                 Loss boundary: 0.00304559082724154,                 Loss phys: 0.040818337351083755\n",
            "Iter 200, Loss: 0.17905452847480774,                 Loss data: 0.1452096700668335,                 Loss boundary: 0.0030615648720413446,                 Loss phys: 0.03078330121934414\n",
            "Iter 300, Loss: 0.14920419454574585,                 Loss data: 0.11549407243728638,                 Loss boundary: 0.0034802884329110384,                 Loss phys: 0.030229832977056503\n",
            "Iter 400, Loss: 0.15845757722854614,                 Loss data: 0.12414795160293579,                 Loss boundary: 0.007842435501515865,                 Loss phys: 0.02646719478070736\n",
            "Iter 500, Loss: 0.1492358148097992,                 Loss data: 0.11656586825847626,                 Loss boundary: 0.007888459600508213,                 Loss phys: 0.02478148601949215\n",
            "Iter 600, Loss: 0.1543724238872528,                 Loss data: 0.1274401843547821,                 Loss boundary: 0.0019023228669539094,                 Loss phys: 0.025029916316270828\n",
            "Iter 700, Loss: 0.15024186670780182,                 Loss data: 0.1268201470375061,                 Loss boundary: 0.005319905001670122,                 Loss phys: 0.01810181513428688\n",
            "Iter 800, Loss: 0.14556515216827393,                 Loss data: 0.12017174810171127,                 Loss boundary: 0.002371792681515217,                 Loss phys: 0.02302161231637001\n",
            "Iter 900, Loss: 0.14942288398742676,                 Loss data: 0.12492929399013519,                 Loss boundary: 0.004394173622131348,                 Loss phys: 0.020099416375160217\n",
            "Iter 1000, Loss: 0.1478988081216812,                 Loss data: 0.12266303598880768,                 Loss boundary: 0.00298857013694942,                 Loss phys: 0.0222471971064806\n",
            "Iter 1100, Loss: 0.15322545170783997,                 Loss data: 0.12928178906440735,                 Loss boundary: 0.00572656374424696,                 Loss phys: 0.01821710728108883\n",
            "Iter 1200, Loss: 0.15910124778747559,                 Loss data: 0.12899744510650635,                 Loss boundary: 0.01055776048451662,                 Loss phys: 0.019546043127775192\n",
            "Iter 1300, Loss: 0.12631139159202576,                 Loss data: 0.10600149631500244,                 Loss boundary: 0.0018830086337402463,                 Loss phys: 0.01842687837779522\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#Train step\u001b[39;00m\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m losses = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtrain_data_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_phys_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_boundary_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mloss_phys_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = losses[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m loss.backward()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(fx, train_data, train_phys, train_bound, loss_phys_func, data_coef, phys_coef, bound_coef)\u001b[39m\n\u001b[32m     26\u001b[39m     x.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m     pF = fx(torch.cat([t, x], dim=\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     loss_phys = \u001b[43mloss_phys_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bound_coef > \u001b[32m0\u001b[39m:\n\u001b[32m     31\u001b[39m     pF = fx(train_bound[:, :\u001b[32m2\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PINNsTuto/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PINNsTuto/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mSinPhyLoss.forward\u001b[39m\u001b[34m(self, pY, t, x, weight, **kwargs)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pY, t, x, weight=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward Function.\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[33;03m            weights. Default: None.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     pd_time = \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpY\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m     pd_pos  = grad(pY.sum(), x, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m     loss    = ((pd_time-\u001b[38;5;28mself\u001b[39m.velocity*pd_pos)**\u001b[32m2.\u001b[39m).mean()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PINNsTuto/.venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/PINNsTuto/.venv/lib64/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "loss_m = 0\n",
        "\n",
        "#Train loop\n",
        "for i in range(N):\n",
        "    #Pick a random batch of data and boundaries\n",
        "    train_data_i = Variable(batch_sample(train_data, BS))\n",
        "    train_boundary_i = Variable(batch_sample(train_boundary, BS))\n",
        "\n",
        "    #Domain points\n",
        "    train_phys_i = Variable(domain_sampling(BS, T, L))\n",
        "\n",
        "    #Train step\n",
        "    optimizer.zero_grad()\n",
        "    losses = train_step(network,\n",
        "                        train_data_i, train_phys_i, train_boundary_i,\n",
        "                        loss_phys_func)\n",
        "    loss_m += losses['loss'].item()\n",
        "    ld = losses['loss_data']\n",
        "    lb = losses['loss_bound']\n",
        "    lp = losses['loss_phys']\n",
        "    losses['loss'].backward()\n",
        "    n = i+1\n",
        "    optimizer.step()\n",
        "    if i % PN == 0:\n",
        "        print(f'Iter {i}, Loss: {loss_m/float(n)}, Loss data: {ld}, Loss boundary: {lb}, Loss phys: {lp}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}